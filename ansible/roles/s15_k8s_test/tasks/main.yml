
# Final testing phase
- name: Deploy aidamian/llm_api as a Deployment
  k8s:
    state: present
    definition:
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: llm-api-deployment
        labels:
          app: llm-api-test
      spec:
        replicas: 3  # Specify the number of replicas
        selector:
          matchLabels:
            app: llm-api-test
        template:
          metadata:
            labels:
              app: llm-api-test
          spec:
            containers:
            - name: llm-api
              image: aidamian/llm_api
              ports:
              - containerPort: 5060
              resources:
                limits:
                  nvidia.com/gpu: 1  # Requesting one GPU per replica
              env:
                - name: MY_VARIABLE
                  value: "value"


- name: Wait for the Deployment to be ready
  k8s_info:
    kind: Deployment
    name: llm-api-deployment
  register: deployment
  until: deployment.resources[0].status.availableReplicas == deployment.resources[0].spec.replicas
  retries: 10
  delay: 6

- name: Get a list of Pods from the Deployment
  k8s_info:
    kind: Pod
    namespace: default
    label_selectors:
      - app=llm-api-test
  register: pod_list


- name: Fetch logs from a Deployment Pod
  shell: kubectl logs {{ pod_list.resources[0].metadata.name }}
  register: pod_logs
  when: pod_list.resources | length > 0

- name: Print pod logs
  debug:
    var: pod_logs.stdout_lines
  when: pod_list.resources | length > 0

- name: Expose the Deployment through a service
  k8s:
    state: present
    definition:
      apiVersion: v1
      kind: Service
      metadata:
        name: llm-api-test-service
      spec:
        selector:
          app: llm-api-test
        ports:
          - protocol: TCP
            port: 5060
            targetPort: 5060
            nodePort: 30060

- name: Wait for the service to be up
  wait_for:
    port: 30060
    delay: 10
    timeout: 60
  delegate_to: localhost

- name: Send a request to the API
  uri:
    url: "http://{{ hostvars['kube_masters']['ansible_host'] }}:30060/predict/"
    method: POST
    body: '{"text": "Tu esti cam f.r.a.i.e.r"}'
    body_format: json
    return_content: yes
    status_code: 200
    headers:
      Content-Type: "application/json"
  register: api_response
  delegate_to: localhost

- name: Display API response
  debug:
    msg: "{{ api_response.json }}"

- name: Assert that CUDA is being used
  assert:
    that:
      - "'cuda' in api_response.json['metadata']['device']"
    fail_msg: "CUDA is not being used in the response"
    success_msg: "CUDA is being used in the response"

- name: Delete test deployment
  k8s:
    state: absent
    definition:
      apiVersion: v1
      kind: Deployment
      metadata:
        name: llm-api-deployment

- name: Delete test service
  k8s:
    state: absent
    definition:
      apiVersion: v1
      kind: Service
      metadata:
        name: llm-api-test-service

- name: Wait for pod deletion
  k8s_info:
    api_version: v1
    kind: Pod
    name: llm-api-deployment
  register: deployment
  until: deployment.resources | length == 0
  retries: 5
  delay: 10

- name: Wait for service deletion
  k8s_info:
    api_version: v1
    kind: Service
    name: llm-api-test-service
  register: service
  until: service.resources | length == 0
  retries: 5
  delay: 10
