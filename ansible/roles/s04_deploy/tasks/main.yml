---
- name: Check if llm_api.service exists
  ansible.builtin.stat:
    path: /etc/systemd/system/llm_api.service
  register: llm_api_service

- name: Gather service facts
  ansible.builtin.service_facts:

- name: Copy -apllmi.service file
  ansible.builtin.template:
    src: llm_api.service.j2
    dest: /etc/systemd/system/llm_api.service
    mode: '0644'
  when: not llm_api_service.stat.exists
  notify: reload systemd

- name: Enable and start llm_api.service
  ansible.builtin.systemd:
    name: llm_api.service
    enabled: yes
    state: started
  when: not llm_api_service.stat.exists or not ('llm_api.service' in services and services['llm_api.service'].status == 'active')


# Task to wait for the container to be ready
- name: Wait for container to be ready
  wait_for:
    port: "{{ llm_api_port }}"  # Wait for port 5050 to be ready
    delay: 20  # Delay before the check starts
    timeout: 60  # Maximum time to wait

# Task to check if the API is working
- name: Check if API is working
  uri:
    url: "http://localhost:{{ llm_api_port }}/predict/"  # API endpoint to test
    method: POST  # HTTP method to use
    body: '{"text": "Tu esti cam f.r.a.i.e.r"}'  # Request body in JSON format
    body_format: json  # Specify the format of the body
    return_content: yes  # Return the content of the response
    status_code: 200  # Expected status code
    headers:
      Content-Type: "application/json"  # Set request header
  register: api_response  # Store the response in a variable

- name: Display API response
  debug:
    msg: "{{ api_response.json }}"

# Task to assert that CUDA is being used
- name: Assert that CUDA is used
  assert:
    that:
      - "'cuda' in api_response.json['metadata']['device']"  # Check if 'cuda' is in the response
    fail_msg: "CUDA is not being used"  # Message to display if assertion fails
    success_msg: "CUDA is being used"  # Message to display if assertion succeeds
